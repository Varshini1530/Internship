{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d12b63d",
   "metadata": {},
   "source": [
    "1)Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a536114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf49dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for scraping data\n",
    "Rank =[]\n",
    "Name =[]\n",
    "Artist =[]\n",
    "Upload_date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank Via X path\n",
    "try:\n",
    "    rank=driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Rank.append('NA')\n",
    "    \n",
    "# Extracting Name Via X path\n",
    "try:\n",
    "    name=driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Name.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Name.append('NA')\n",
    "    \n",
    "# Extracting Artist Name Via Xpath\n",
    "try:\n",
    "    artist=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\")\n",
    "    for i in artist:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Artist.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Artist.append('NA')\n",
    "    \n",
    "# Extracting Upload date Via Xpath\n",
    "try:\n",
    "    upload_date=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\")\n",
    "    for i in upload_date:\n",
    "        Upload_date.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Upload_date.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Upload_date.append('NA') \n",
    "\n",
    "# Extracting Views via Xpath\n",
    "try:\n",
    "    views=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\")\n",
    "    for i in views:\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Views.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Views.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for scrap data\n",
    "Wiki_YT=pd.DataFrame({'Rank':Rank,'Video Name':Name,'Uploader':Artist,'Views (in Billons)':Views,'Upload Date':Upload_date})\n",
    "print('\\033[1m'+'Most Viewed Video on YouTube from Wikipedia :'+'\\033[0m')\n",
    "Wiki_YT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b862d7",
   "metadata": {},
   "source": [
    "2)Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.bcci.tv/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening fixtures webpage through browser\n",
    "fixtures=driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']/div/ul/li/a\")\n",
    "try:\n",
    "    fixtures.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(fixtures.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping URL \n",
    "URL= []\n",
    "link=driver.find_elements_by_xpath('//div[@class=\"js-list\"]/a')\n",
    "for i in link:\n",
    "    URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "        \n",
    "    # Scraping Series Via Xpath\n",
    "    try:\n",
    "        series=driver.find_element_by_xpath(\"//h1[@class='mc-header-info__title']/strong\")\n",
    "        Series.append(series.text)\n",
    "    except NoSuchElementException:\n",
    "        Series.append('NA')\n",
    "        \n",
    "    # Scraping venue Via Xpath\n",
    "    try:\n",
    "        place=driver.find_element_by_xpath(\"//h1[@class='mc-header-info__title']\")\n",
    "        Place.append(place.text)\n",
    "    except NoSuchElementException:\n",
    "        Place.append('NA')\n",
    "        \n",
    "    # Scraping date Via Xpath\n",
    "    try:\n",
    "        date=driver.find_element_by_xpath('//div[@class=\"mc-header-scorebox__datetime\"]/strong')\n",
    "        Date.append(date.text)\n",
    "    except NoSuchElementException:\n",
    "        Date.append('NA')\n",
    "        \n",
    "    # Scraping Time Via Xpath\n",
    "    try:\n",
    "        time=driver.find_element_by_xpath('//div[@class=\"mc-header-scorebox__datetime\"]/span')\n",
    "        Time.append(time.text)\n",
    "    except NoSuchElementException:\n",
    "        Time.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for scrap data\n",
    "Cricket=pd.DataFrame({'Tournament':Series,'Venue':Place,'Date':Date,'Time':Time})\n",
    "print('\\033[1m'+'Team India’s Upcoming International fixtures :'+'\\033[0m')\n",
    "Cricket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d6396",
   "metadata": {},
   "source": [
    "3)Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.statisticstimes.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty lists\n",
    "Rank =[]\n",
    "State =[]\n",
    "GDSP_18_19=[]\n",
    "GDSP_19_20 =[]\n",
    "Share =[]\n",
    "GDP =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on India under ecomony section\n",
    "India=driver.find_element_by_xpath('//div[@class=\"dropdown\"][2]/div/a[3]')\n",
    "try:\n",
    "    India.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(India.get_attribute('href'))\n",
    "    \n",
    "time.sleep(2)\n",
    "\n",
    "# Clicking Indian State GDP\n",
    "state = driver.find_element_by_xpath('//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li[1]/a')\n",
    "try:\n",
    "    state.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(state.get_attribute('href'))\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank via Xpath\n",
    "try:\n",
    "    rank=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('NA')\n",
    "    \n",
    "# Extracting state name via Xpath\n",
    "try:\n",
    "    state=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "    for i in state:\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append('NA')\n",
    "    \n",
    "# Extracting GDSP 18-19 via Xpath\n",
    "try:\n",
    "    gdsp_18_19=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "    for i in gdsp_18_19:\n",
    "        GDSP_18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDSP_18_19.append('NA')\n",
    "    \n",
    "# Extracting GDSP 19-20 via Xpath\n",
    "try:\n",
    "    gdsp_19_20=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "    for i in gdsp_19_20:\n",
    "        GDSP_19_20.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDSP_19_20.append('NA')\n",
    "    \n",
    "# Extracting share in billons via Xpath\n",
    "try:\n",
    "    share=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "    for i in share:\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append('NA')\n",
    "\n",
    "# Extracting GDP via Xpath\n",
    "try:\n",
    "    gdp=driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "    for i in gdp:\n",
    "        GDP.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for scrap data\n",
    "Statewise_GDP=pd.DataFrame({'Rank':Rank,'State':State,'GDSP 18-19':GDSP_18_19,'GDSP 19-20':GDSP_19_20,\n",
    "                            'Share':Share,'GDP (in Billions)':GDP})\n",
    "print('\\033[1m'+'State-wise GDP of India :'+'\\033[0m')\n",
    "Statewise_GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a691804",
   "metadata": {},
   "source": [
    "4)Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627927c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://github.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on trending option\n",
    "trending=driver.find_element_by_xpath('//ul[@class=\"list-style-none mb-3\"][2]/li[3]/a')\n",
    "try:\n",
    "    trending.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty list for Scraping Data\n",
    "Title=[]\n",
    "Description=[]\n",
    "Count =[]\n",
    "Language =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping repositories URL\n",
    "URL= []\n",
    "link=driver.find_elements_by_xpath('//h1[@class=\"h3 lh-condensed\"]/a')\n",
    "for i in link:\n",
    "    URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6463462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Repositority Title\n",
    "try:\n",
    "    title=driver.find_elements_by_xpath('//h1[@class=\"h3 lh-condensed\"]')\n",
    "    for i in title:\n",
    "        Title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Title.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    # Scraping Description\n",
    "    try:\n",
    "        description=driver.find_element_by_xpath('/html/body/div[4]/div/main/div[2]/div/div/div[2]/div[2]/div/div[1]/div/p')\n",
    "        Description.append(description.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('NA')\n",
    "    \n",
    "    # Scraping Contributor count\n",
    "    try:\n",
    "        count=driver.find_element_by_xpath(\"//h2[@class='h4 mb-3']/a[contains(text(),'Contributors')]/span\")\n",
    "        Count.append(count.text)\n",
    "    except NoSuchElementException:\n",
    "        Count.append('NA')\n",
    "    \n",
    "    # Scraping Language\n",
    "    L =[]\n",
    "    try:\n",
    "        lang=driver.find_elements_by_xpath(\"//li[@class='d-inline']//a//span[1]\")\n",
    "        if lang:\n",
    "            for j in lang:\n",
    "                L.append(j.text)\n",
    "        else:\n",
    "            L.append('NA')\n",
    "        Language.append(L)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Github=pd.DataFrame({'Title':Title,'Description':Description,\n",
    "                'Contributors_count':Count,'Language_used':Language})\n",
    "print('\\033[1m'+'GitHub Trending Repository :'+'\\033[0m')\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a257bb5",
   "metadata": {},
   "source": [
    "5)Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.billboard.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa375780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty List\n",
    "Song =[]\n",
    "Artist =[]\n",
    "Last_Week_rank=[]\n",
    "Peak_rank =[]\n",
    "Weeks =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on hot 100 option\n",
    "hot_100=driver.find_element_by_xpath('//ul[@class=\"header__subnav__list display--flex flex--left-center flex-md--center-center\"]/li[2]/a')\n",
    "try:\n",
    "    hot_100.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(hot_100.get_attribute('href'))\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Song Name\n",
    "try:\n",
    "    song=driver.find_elements_by_xpath('//span[@class=\"chart-element__information\"]/span[1]')\n",
    "    for i in song:\n",
    "        Song.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Song.append('NA')\n",
    "    \n",
    "# Scraping Song Artist Name\n",
    "try:\n",
    "    artist=driver.find_elements_by_xpath('//span[@class=\"chart-element__information\"]/span[2]')\n",
    "    for i in artist:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append('NA')\n",
    "    \n",
    "# Scraping Song last week rank\n",
    "try:\n",
    "    last_rank=driver.find_elements_by_xpath('//span[@class=\"chart-element__information\"]/span[2]')\n",
    "    for i in last_rank:\n",
    "        Last_Week_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Last_Week_rank.append('NA')\n",
    "\n",
    "# Scraping Song peak rank\n",
    "try:\n",
    "    peak=driver.find_elements_by_xpath('//span[@class=\"chart-element__information\"]/span[2]')\n",
    "    for i in peak:\n",
    "        Peak_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Peak_rank.append('NA')\n",
    "\n",
    "# Scraping Song Artist Name\n",
    "try:\n",
    "    weeks=driver.find_elements_by_xpath('//span[@class=\"chart-element__information\"]/span[2]')\n",
    "    for i in weeks:\n",
    "        Weeks.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Weeks.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b64512",
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard=pd.DataFrame({'Song_Name':Song,\n",
    "                'Artist_Name':Artist,\n",
    "                'Last_week_rank':Last_Week_rank,\n",
    "                'Peak':Peak_rank,\n",
    "                'Weeks_on_chart':Weeks})\n",
    "print('\\033[1m'+'Billboard Hot 100 Songs :'+'\\033[0m')\n",
    "Billboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb4d45",
   "metadata": {},
   "source": [
    "6)Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d517c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d23303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19531f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty Lists\n",
    "Book =[]\n",
    "Author =[]\n",
    "Volumes_sold =[]\n",
    "Publisher =[]\n",
    "Genre =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Book name\n",
    "try:\n",
    "    book=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "    for i in book:\n",
    "        Book.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Book.append('NA')\n",
    "    \n",
    "# Scraping Book author's via Xpath\n",
    "try:\n",
    "    author=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "    for i in author:\n",
    "        Author.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Author.append('NA')\n",
    "    \n",
    "# Scraping Volumes sold via Xpath\n",
    "try:\n",
    "    sold=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "    for i in sold:\n",
    "        Volumes_sold.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Volumes_sold.append('NA')\n",
    "    \n",
    "# Scraping publisher via xPath\n",
    "try:\n",
    "    publisher=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "    for i in publisher:\n",
    "        Publisher.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Publisher.append('NA')\n",
    "    \n",
    "# Scraping Genre via Xpath\n",
    "try:\n",
    "    genre=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "    for i in genre:\n",
    "        Genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Genre.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36095b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "Top_Books=pd.DataFrame({\"Book Title\":Book,\n",
    "                \"Author Name\":Author,\n",
    "                'Volumes sold':Volumes_sold,\n",
    "                'Publisher':Publisher,\n",
    "                'Genre':Genre})\n",
    "print('\\033[1m'+'Best Selling Books of All Time List by The Guardian  :'+'\\033[0m')\n",
    "Top_Books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c22268",
   "metadata": {},
   "source": [
    "7)Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b481093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Empty List\n",
    "Name =[]\n",
    "Year_Span=[]\n",
    "Genre =[]\n",
    "Run_Time =[]\n",
    "Ratings =[]\n",
    "Votes =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Name via Xpath\n",
    "try:\n",
    "    name=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\")\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append('NA')\n",
    "\n",
    "# Scraping Year span via Xpath\n",
    "try:\n",
    "    year=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/span[2]\")\n",
    "    for i in year:\n",
    "        Year_Span.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Year_Span.append('NA')\n",
    "    \n",
    "# Scraping Genre via Xpath\n",
    "try:\n",
    "    genre=driver.find_elements_by_xpath(\"//span[@class='genre']\")\n",
    "    for i in genre:\n",
    "        Genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Genre.append('NA')\n",
    "    \n",
    "# Scraping RunTime via Xpath\n",
    "try:\n",
    "    runtime=driver.find_elements_by_xpath(\"//span[@class='runtime']\")\n",
    "    for i in runtime:\n",
    "        Run_Time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Run_Time.append('NA')\n",
    "\n",
    "# Scraping Ratings via Xpath\n",
    "try:\n",
    "    ratings=driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "    for i in ratings:\n",
    "        Ratings.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Ratings.append('NA')\n",
    "\n",
    "# Scraping Votes via Xpath\n",
    "try:\n",
    "    votes=driver.find_elements_by_xpath(\"//span[@name='nv']\")\n",
    "    for i in votes:\n",
    "        Votes.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Votes.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "IMDB_TV=pd.DataFrame({\"Name\":Name,\n",
    "                \"Year Span\":Year_Span,\n",
    "                \"Genre\":Genre,\n",
    "                \"Run Time\":Run_Time,\n",
    "                \"Ratings\":Ratings,\n",
    "                \"Votes\":Votes})\n",
    "print('\\033[1m'+'Top 100 most watched tv shows of all time IMDB :'+'\\033[0m')\n",
    "IMDB_TV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e0dfe",
   "metadata": {},
   "source": [
    "8)Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute \n",
    "G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    "ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb30603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://archive.ics.uci.edu/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on all datasets links\n",
    "dataset=driver.find_element_by_xpath(\"//a[@href='datasets.php']\")\n",
    "dataset.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e781e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists\n",
    "Dataset =[]\n",
    "Data_Type =[]\n",
    "Task =[]\n",
    "Attribute_Type =[]\n",
    "No_of_Instances =[]\n",
    "No_of_Attribute =[]\n",
    "Year =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping DataSet Name via Xpath\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    dataset=driver.find_elements_by_xpath(\"//p[@class='normal']/b/a\")\n",
    "    for i in tqdm(dataset):\n",
    "        Dataset.append(i.text)\n",
    "        time.sleep(0.15)\n",
    "except NoSuchElementException:\n",
    "    Dataset.append('NA')\n",
    "    \n",
    "# Scraping Data Type via Xpath\n",
    "try:\n",
    "    Type=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]')\n",
    "    for i in Type[1:]:\n",
    "        Data_Type.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Data_Type.append('NA')\n",
    "    \n",
    "# Scraping Task via Xpath\n",
    "try:\n",
    "    task=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]')\n",
    "    for i in task[1:]:\n",
    "        Task.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Task.append('NA')\n",
    "    \n",
    "# Scraping Attribute_Type via Xpath\n",
    "try:\n",
    "    attribute_Type=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]')\n",
    "    for i in attribute_Type[1:]:\n",
    "        Attribute_Type.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Attribute_Type.append('NA')\n",
    "    \n",
    "# Scraping No_of_Instances via Xpath\n",
    "try:\n",
    "    no_of_Instances=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]')\n",
    "    for i in no_of_Instances[1:]:\n",
    "        No_of_Instances.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    No_of_Instances.append('NA')\n",
    "\n",
    "# Scraping No_of_Attribute via Xpath\n",
    "try:\n",
    "    no_of_Attribute=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]')\n",
    "    for i in no_of_Attribute[1:]:\n",
    "        No_of_Attribute.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    No_of_Attribute.append('NA')\n",
    "    \n",
    "# Scraping Year via Xpath\n",
    "try:\n",
    "    year=driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]')\n",
    "    for i in year[1:]:\n",
    "        Year.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Year.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6737ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Dataset),len(Data_Type),len(Task),len(Attribute_Type),len(No_of_Instances),len(Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Data Frame for UCI Dataset\n",
    "UCI_Dataset=pd.DataFrame({'DataSet Title':Dataset,\n",
    "                'Data Type':Data_Type,\n",
    "                'Task':Task,\n",
    "                'Attribute Type':Attribute_Type,\n",
    "                'No of Instances':No_of_Instances,\n",
    "                'No of Attribute':No_of_Attribute,\n",
    "                'Year':Year})\n",
    "print('\\033[1m'+' UCI Machine Learning Dataset:'+'\\033[0m')\n",
    "UCI_Dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
