{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f5c2b4",
   "metadata": {},
   "source": [
    "1)Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24edd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5690e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the webdriver\n",
    "driver=webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "time.sleep(2)\n",
    "\n",
    "# Opening Amazon.in in chrome browser\n",
    "url='http://www.amazon.in/'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# Taking input from user about product search\n",
    "User_input=input('Enter the title of Product you are interest in search :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_xpath('//input[@id=\"twotabsearchtextbox\"]')\n",
    "# clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input specified by user to search menu through send keys\n",
    "Search.send_keys(User_input)\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_xpath('//input[@id=\"nav-search-submit-button\"]')\n",
    "# Clicking search button\n",
    "Search_button.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fa8d1",
   "metadata": {},
   "source": [
    "2)In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Empty list to scrap data\n",
    "Brand =[]\n",
    "Product = []\n",
    "Price =[]\n",
    "Return =[]\n",
    "Excepted_delivery =[]\n",
    "Availability =\n",
    "Other_details =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL= []\n",
    "# range(0,3) used to scrape three pages on website\n",
    "for page in range(0,3):\n",
    "    url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for i in url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "    # locating next page button and clicking\n",
    "    Nxt_page=driver.find_element_by_xpath('//li[@class=\"a-last\"][1]/a')\n",
    "    Nxt_page.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extracting Brand Name via Xpath\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')\n",
    "        Brand.append(brand.text) \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting product name via Xpath\n",
    "    try:\n",
    "        product =driver.find_element_by_xpath('//span[@id=\"productTitle\"]')\n",
    "        Product.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        Product.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Price via Xpath\n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//span[@id=\"priceblock_dealprice\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Return or exchange detail via Xpath\n",
    "    try:\n",
    "        replacement=driver.find_element_by_xpath('//*[@id=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        Return.append(replacement.text)\n",
    "    except NoSuchElementException:\n",
    "        Return.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Expected Delivery via Xpath\n",
    "    try:\n",
    "        delivery=driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')\n",
    "        Excepted_delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        Excepted_delivery.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Availability via Xath\n",
    "    try:\n",
    "        availability=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-success\"]')\n",
    "        Availability.append(availability.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting other details via Xpath\n",
    "    try:\n",
    "        other=driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        Other_details.append(other.text)\n",
    "    except NoSuchElementException:\n",
    "        Other_details.append('-')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guitar=pd.DataFrame({'Brand':Brand,'Product':Product,'Price':Price,\n",
    "                        'Return/Exchange':Return,'Expected Delivery':Excepted_delivery,'Availability':Availability,\n",
    "                        'Other Details':Other_details,'URL':URL})\n",
    "print('\\033[1m'+'Amazon Festive Sale Guitar with exciting offers :'+'\\033[0m')\n",
    "Guitar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a1589",
   "metadata": {},
   "source": [
    "3)Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d7109",
   "metadata": {},
   "source": [
    "Searching and extracting for Fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in Chrome browser\n",
    "url='https://images.google.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d33ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Fruits' in search bar\n",
    "Search.send_keys('Fruits')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_class_name('zgAlFc')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89966111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 25000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,25000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6213d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Fruits\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f2c8e",
   "metadata": {},
   "source": [
    "Searching and extracting for Cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Cars')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_class_name('zgAlFc')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a04d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 25000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,50000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391da75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Machine Learning\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38704d",
   "metadata": {},
   "source": [
    "4)Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa73816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing require libary\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in Chrome browser\n",
    "url='http://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    login_window = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "    login_window.click()\n",
    "except NoSuchElementException:\n",
    "    print('Login Window is not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by xpath\n",
    "Search=driver.find_element_by_xpath('//div[@class=\"_3OO5Xc\"]/input')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Oneplus' in search bar\n",
    "Search.send_keys('Iphone Mobile')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making empty list to scrape data\n",
    "Brand =[]\n",
    "Smartphone =[]\n",
    "Colour =[]\n",
    "Storage_Rom =[]\n",
    "Primary_camera=[]\n",
    "Secondary_camera=[]\n",
    "Display_size=[]\n",
    "Display_resolution=[]\n",
    "Processor =[]\n",
    "Processor_cores=[]\n",
    "Battery_capacity=[]\n",
    "Price =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80770c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "url=driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for i in url:\n",
    "    Href=i.get_attribute('href')\n",
    "    URL.append(Href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e80bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Expanding specification table by clicking on read more button\n",
    "    try:\n",
    "        Read_more=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')\n",
    "        Read_more.click()\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print('NoSuchElementException Occur')\n",
    "        pass\n",
    "    \n",
    "    # Extracting Brand Name via Xpath\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')\n",
    "        Brand.append(brand.text.split()[0])\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        \n",
    "    # Extracting Smartphone Model via Xpath\n",
    "    try:\n",
    "        smartphone=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Smartphone.append(smartphone.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Smartphone.append('-')\n",
    "        \n",
    "    # Extracting Colour via Xpath\n",
    "    try:\n",
    "        colour=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Colour.append(colour.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')\n",
    "    \n",
    "    # Extracting Storage Rom via Xpath\n",
    "    try:\n",
    "        Rom=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Storage_Rom.append(Rom.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Storage_Rom.append('Not Mention')\n",
    "        \n",
    "    # Extracting primary camera detail via Xpath\n",
    "    try:\n",
    "        primary_camera=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Primary_camera.append(primary_camera.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Primary_camera.append('Not Mention')\n",
    "        \n",
    "    # Extracting Secondary Camera detail via Xpath\n",
    "    try:\n",
    "        secondary_cam=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table/tbody/tr[5]/td[2]/ul/li')\n",
    "        Secondary_camera.append(secondary_cam.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Secondary_camera.append('Not Mention')\n",
    "    \n",
    "    # Extracting Display size via Xpath\n",
    "    try:\n",
    "        display=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_size.append(display.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_size.append('-')\n",
    "    \n",
    "    # Extracting Display Resolution via Xpath\n",
    "    try:\n",
    "        resolution=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Display_resolution.append(resolution.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_resolution.append('-')\n",
    "    \n",
    "    # Extracting processor detail via xpath\n",
    "    try:\n",
    "        processor=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Processor.append(processor.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Processor.append('-')\n",
    "        \n",
    "    # Extracting Processor core via xpath\n",
    "    try:\n",
    "        core=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Processor_cores.append(core.text)\n",
    "    except NoSuchElementException:\n",
    "        Processor_cores.append('Not Mention')\n",
    "    \n",
    "    #Extracting Price via X path\n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('Not Mention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cf6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iphone=pd.DataFrame({'Brand':Brand,'Smartphone':Smartphone,'Colour':Colour,'Price':Price,\n",
    "                        'Storage Rom':Storage_Rom,'Primary Camera':Primary_camera,'Display Size':Display_size,\n",
    "                        'Display Resolution':Display_resolution,'Processor':Processor,'Processor Cores':Processor_cores})\n",
    "print('\\033[1m'+'Flipkart Iphone Models :'+'\\033[0m')\n",
    "Iphone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574751e",
   "metadata": {},
   "source": [
    "5)Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a22edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5463b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.google.co.in/maps'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4da04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_id(\"searchboxinput\") \n",
    "# clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input specified by user to search menu through send keys\n",
    "Search.send_keys('Nashik')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_id(\"searchbox-searchbutton\")  \n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_url=driver.current_url\n",
    "print('Current url :',current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"@\" in current_url:\n",
    "        location=current_url.split('@')[1].split(',*/data')[0].split(',')\n",
    "        location\n",
    "        print('Latitude of given Location:',location[0])\n",
    "        print('Longitude of given Location:',location[1])\n",
    "except:\n",
    "    print('Location detail not found in url')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa102e3",
   "metadata": {},
   "source": [
    "6)Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.digit.in'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2704746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on top 10 option button in menu bar\n",
    "driver.find_element_by_xpath('//div[@class=\"menu\"]/ul/li[4]/a').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Clicking on Laptop under table\n",
    "driver.find_element_by_xpath('//div[@class=\"categoty_list\"]/button[2]').click()\n",
    "\n",
    "# Opening Best gaming laptops link\n",
    "Gaming_lappy=driver.find_element_by_xpath('//div[@id=\"laptops\"]/div[3]/a').get_attribute('href')\n",
    "driver.get(Gaming_lappy)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Empty list for scraping data\n",
    "Laptop_model =[]\n",
    "OS =[]\n",
    "Display = []\n",
    "Processor =[]\n",
    "Memory =[]\n",
    "Weight =[]\n",
    "Dimension =[]\n",
    "Graphics_processor =[]\n",
    "Price =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop model via Xpath\n",
    "for i in range(0,11):\n",
    "    laptop_model=driver.find_elements_by_xpath('//*[@id=\"toptenIdevent{}\"]/a/h3'.format(i))\n",
    "    for j in laptop_model:\n",
    "       Laptop_model.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting OS on Laptop model via Xpath\n",
    "os=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[3]/td[3]')\n",
    "for j in os:\n",
    "    OS.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e692ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Display vertical via Xpath\n",
    "display=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[4]/td[3]')\n",
    "for j in display:\n",
    "    Display.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a54cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Processor via xpath\n",
    "processor=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[5]/td[3]')\n",
    "for j in processor:\n",
    "    Processor.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Memory via xpath\n",
    "memory=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[6]/td[3]')\n",
    "for j in memory:\n",
    "    Memory.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop Weight via xpath\n",
    "weight=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[7]/td[3]')\n",
    "for j in weight:\n",
    "    Weight.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Dimension via xpath\n",
    "dimension=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[8]/td[3]')\n",
    "for j in dimension:\n",
    "    Dimension.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Graphics Processor type via xpath\n",
    "graphics_processor=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[9]/td[3]')\n",
    "for j in graphics_processor:\n",
    "    Graphics_processor.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85173e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop Price type via xpath\n",
    "price=driver.find_elements_by_xpath('//table[@id=\"summtable\"]//tr//td[3]')\n",
    "for j in price:\n",
    "    Price.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affeafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make data frame\n",
    "Gaming_Laptops=pd.DataFrame({\"Laptop Model\":Laptop_model,\"Price\":Price, \"OS\":OS,\"Display\":Display,\"Memory\":Memory,\"Processor\":Processor,\n",
    "                 \"Weight\":Weight,\"Dimension\":Dimension,\"Graphical processor\":Graphics_processor})\n",
    "print('\\033[1m'+'Best Gaming Laptop :'+'\\033[0m')\n",
    "Gaming_Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07c4fe",
   "metadata": {},
   "source": [
    "7)Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87dfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.forbes.com\"\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on explore button to expand menu\n",
    "driver.find_element_by_xpath('//button[@class=\"icon--hamburger\"]').click()\n",
    "\n",
    "# Clicking on Billionaire category\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]').click()\n",
    "\n",
    "# Clicking on world billionaire tab\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]/div[2]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to scrap data\n",
    "Rank =[]\n",
    "Name=[]\n",
    "Net_worth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b567386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank No\n",
    "rank=driver.find_elements_by_xpath('//div[@class=\"rank\"]')\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef79d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extacting name\n",
    "name=driver.find_elements_by_xpath('//div[@class=\"personName\"]/div')\n",
    "for i in name:\n",
    "    Name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Net worth\n",
    "net=driver.find_elements_by_xpath('//div[@class=\"netWorth\"]/div')\n",
    "for i in net:\n",
    "    Net_worth.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Age\n",
    "age=driver.find_elements_by_xpath('//div[@class=\"age\"]/div')\n",
    "for i in age:\n",
    "    Age.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extracting Citizenship\n",
    "citizen=driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]')\n",
    "for i in citizen:\n",
    "    Citizenship.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de822a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Source\n",
    "try:\n",
    "    source=driver.find_elements_by_xpath('//div[@class=\"source-column\"]')\n",
    "    for i in source:\n",
    "        Source.append(i.text)\n",
    "except:\n",
    "    source=driver.find_elements_by_xpath('//*[@id=\"jeff-bezos\"]/div[6]/div/div[1]')\n",
    "    Source.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Industry\n",
    "industry=driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "for i in industry:\n",
    "    Industry.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14205f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "Forbes_list=pd.DataFrame({'Rank':Rank,'Name':Name,'Net Worth':Net_worth,'Age':Age,'Citizenship':Citizenship,\n",
    "                'Source':Source,'Industry':Industry})\n",
    "print('\\033[1m'+' Forbes World Billionaires List Oct 2021 :'+'\\033[0m')\n",
    "Forbes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1aef0",
   "metadata": {},
   "source": [
    "8)Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2081b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.youtube.com\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542825f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_xpath('//input[@id=\"search\"]') \n",
    "# Feeding input video name by user to search menu through send keys\n",
    "Search.send_keys('Healing Ragas - Sitar Tabla - Brindavan Sarang - Classical Instrumental Fusion B.Sivaramakrishna Rao')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_xpath('//button[@id=\"search-icon-legacy\"]/yt-icon')  \n",
    "# Clicking search button\n",
    "Search_button.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on video\n",
    "link = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 15000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,1000000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to scrap data\n",
    "Comments = []\n",
    "Comment_posted_ago = []\n",
    "Timeline = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting comment on video\n",
    "comment = driver.find_elements_by_id(\"content-text\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(comment):\n",
    "    if i.text is None:\n",
    "        Comments.append(\"--\")\n",
    "    else:\n",
    "        Comments.append(i.text)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time of commenting\n",
    "timeline = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(timeline):\n",
    "    if i.text is None:\n",
    "        Timeline.append(\"-\")\n",
    "    else:\n",
    "        Timeline.append(i.text)\n",
    "for i in tqdm(range(0,len(Timeline),2)):\n",
    "    Comment_posted_ago.append(Timeline[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef894df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Comments),len(Comment_posted_ago),len(No_of_Likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0482dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YT=pd.DataFrame({})\n",
    "YT['Comments']=Comments[:500]\n",
    "YT['Comment Posted Ago']=Comment_posted_ago[:500]\n",
    "YT['No. of Likes']=No_of_Likes[:500]\n",
    "YT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2e33d",
   "metadata": {},
   "source": [
    "9)Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.hostelworld.com\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4431206",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"page-footer-accomodation\"]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb75de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "Search = driver.find_element_by_xpath('//input[@id=\"home-search-keywords\"]')\n",
    "\n",
    "# Sending input London in search bar\n",
    "Search.send_keys(\"London\")\n",
    "time.sleep(1)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[1]/div[2]/ul/li[2]')\n",
    "london.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# do click on search button\n",
    "Search_button = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[2]/button')\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "    #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "    #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "data = list(zip(hostel_name,distance,rating,reviews,over_all,pvt_prices,dorms_price,facilities,description))       \n",
    "Hostel = pd.DataFrame(data, columns = [\"Hostel name\",\"Distance from city centre\",\"ratings\",\"Total reviews\",\"Overall review\",\"Privates from price\",\"Dorms from price\",\"Facilities\",\"Property Description\"])\n",
    "print('\\033[1m'+' Hostel Available in London :'+'\\033[0m')\n",
    "Hostel.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
