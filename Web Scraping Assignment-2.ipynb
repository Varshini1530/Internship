{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac103d6",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment-2  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f123f1",
   "metadata": {},
   "source": [
    "1) Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Shine.com\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Find the search fields and input data\n",
    "job_title_input = driver.find_element_by_id(BY.CLASS_NAME,\"searchBarInput\")\n",
    "location_input = driver.find_element_by_id(BY.CLASS_NAME,\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\")\n",
    "search_button = driver.find_element_by_id(BY.CLASS_NAME,\"iconH-zoom mr-30\")\n",
    "\n",
    "\n",
    "# Enter \"Data Analyst\" in job title and \"Bangalore\" in location\n",
    "job_title_input.send_keys(\"Data Analyst\")\n",
    "location_input.send_keys(\"Bangalore\")\n",
    "\n",
    "# Click the search button\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Scrape the data for the first 10 jobs\n",
    "job_results = driver.find_elements_by_class_name(\"search-result\")\n",
    "for job_result in job_results[:10]:\n",
    "    job_html = job_result.get_attribute(\"innerHTML\")\n",
    "    soup = BeautifulSoup(job_html, \"html.parser\")\n",
    "    \n",
    "    # Extract job title\n",
    "    title = soup.find(\"div\", class_=\"job-title\")\n",
    "    job_titles.append(title.text.strip())\n",
    "    \n",
    "    # Extract job location\n",
    "    location = soup.find(\"span\", class_=\"job-location\")\n",
    "    job_locations.append(location.text.strip())\n",
    "    \n",
    "    # Extract company name\n",
    "    company = soup.find(\"span\", class_=\"company-name\")\n",
    "    company_names.append(company.text.strip())\n",
    "    \n",
    "    # Extract experience required\n",
    "    experience = soup.find(\"span\", class_=\"exp\")\n",
    "    experience_required.append(experience.text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Job Title\": job_titles,\n",
    "    \"Job Location\": job_locations,\n",
    "    \"Company Name\": company_names,\n",
    "    \"Experience Required\": experience_required\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688dc3b",
   "metadata": {},
   "source": [
    "2)Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab979322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Shine.com\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Find the search fields and input data\n",
    "job_title_input = driver.find_element_by_id(\"query\")\n",
    "location_input = driver.find_element_by_id(\"location\")\n",
    "search_button = driver.find_element_by_id(\"btnSearch\")\n",
    "\n",
    "# Enter \"Data Scientist\" in job title and \"Bangalore\" in location\n",
    "job_title_input.send_keys(\"Data Scientist\")\n",
    "location_input.send_keys(\"Bangalore\")\n",
    "\n",
    "# Click the search button\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "\n",
    "# Scrape the data for the first 10 jobs\n",
    "job_results = driver.find_elements_by_class_name(\"search-result\")\n",
    "for job_result in job_results[:10]:\n",
    "    job_html = job_result.get_attribute(\"innerHTML\")\n",
    "    soup = BeautifulSoup(job_html, \"html.parser\")\n",
    "    \n",
    "    # Extract job title\n",
    "    title = soup.find(\"div\", class_=\"job-title\")\n",
    "    job_titles.append(title.text.strip())\n",
    "    \n",
    "    # Extract job location\n",
    "    location = soup.find(\"span\", class_=\"job-location\")\n",
    "    job_locations.append(location.text.strip())\n",
    "    \n",
    "    # Extract company name\n",
    "    company = soup.find(\"span\", class_=\"company-name\")\n",
    "    company_names.append(company.text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Job Title\": job_titles,\n",
    "    \"Job Location\": job_locations,\n",
    "    \"Company Name\": company_names\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22922c92",
   "metadata": {},
   "source": [
    "3)In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9524d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Shine.com\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Find the search field and input data\n",
    "job_search_input = driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "job_search_input.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Click the search button\n",
    "search_button = driver.find_element_by_class_name(\"qsb-searchbtn\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the results to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Apply location filter \"Delhi/NCR\"\n",
    "location_filter = driver.find_element_by_xpath(\"//label[@for='chk-Delhi/NCR-cityType-New Delhi']\")\n",
    "location_filter.click()\n",
    "\n",
    "# Apply salary filter \"3-6 lakhs\"\n",
    "salary_filter = driver.find_element_by_xpath(\"//label[@for='chk-3-6 Lakhs-ctcFilter-3-6-0']\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Wait for the filters to be applied\n",
    "time.sleep(2)\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Scrape the data for the first 10 jobs\n",
    "job_results = driver.find_elements_by_class_name(\"search-result\")\n",
    "for job_result in job_results[:10]:\n",
    "    job_html = job_result.get_attribute(\"innerHTML\")\n",
    "    soup = BeautifulSoup(job_html, \"html.parser\")\n",
    "    \n",
    "    # Extract job title\n",
    "    title = soup.find(\"div\", class_=\"job-title\")\n",
    "    job_titles.append(title.text.strip())\n",
    "    \n",
    "    # Extract job location\n",
    "    location = soup.find(\"span\", class_=\"job-location\")\n",
    "    job_locations.append(location.text.strip())\n",
    "    \n",
    "    # Extract company name\n",
    "    company = soup.find(\"span\", class_=\"company-name\")\n",
    "    company_names.append(company.text.strip())\n",
    "    \n",
    "    # Extract experience required\n",
    "    experience = soup.find(\"span\", class_=\"exp\")\n",
    "    experience_required.append(experience.text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Job Title\": job_titles,\n",
    "    \"Job Location\": job_locations,\n",
    "    \"Company Name\": company_names,\n",
    "    \"Experience Required\": experience_required\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473d8dc",
   "metadata": {},
   "source": [
    "4)Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. ProductDescription\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311e88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seleniumNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "     -------------------------------------- 400.2/400.2 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.4 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c35614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Brand                                Product Description   Price\n",
      "0            IRUS             UV Protection Wayfarer Sunglasses (56)    ₹474\n",
      "1            IRUS             UV Protection Wayfarer Sunglasses (53)    ₹534\n",
      "2       Elligator    UV Protection Aviator, Wayfarer Sunglasses (54)    ₹156\n",
      "3            SRPM             UV Protection Wayfarer Sunglasses (50)    ₹149\n",
      "4       Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...    ₹149\n",
      "..            ...                                                ...     ...\n",
      "95       PROVOGUE   UV Protection, Gradient Wayfarer Sunglasses (55)    ₹645\n",
      "96    Eyewearlabs  Polarized, UV Protection Retro Square Sunglass...  ₹1,599\n",
      "97  VINCENT CHASE  by Lenskart Polarized, UV Protection Round Sun...    ₹883\n",
      "98      Elligator                UV Protection Round Sunglasses (54)    ₹249\n",
      "99  VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...    ₹699\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize variables to store data\n",
    "brands = []\n",
    "product_descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Initialize page counter\n",
    "page = 1\n",
    "\n",
    "# Continue scraping until we have data for 100 sunglasses\n",
    "while len(brands) < 100:\n",
    "    # Define the URL for the search results page\n",
    "    url = f\"https://www.flipkart.com/search?q=sunglasses&page={page}\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all the sunglasses listings on the page\n",
    "        listings = soup.find_all(\"div\", class_=\"_2B099V\")\n",
    "        \n",
    "        for listing in listings:\n",
    "            # Extract Brand\n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\")\n",
    "            brands.append(brand.text.strip())\n",
    "            \n",
    "            # Extract Product Description\n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\")\n",
    "            product_descriptions.append(description.text.strip())\n",
    "            \n",
    "            # Extract Price\n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\")\n",
    "            prices.append(price.text.strip())\n",
    "            \n",
    "            # Break if we have data for 100 sunglasses\n",
    "            if len(brands) == 100:\n",
    "                break\n",
    "    \n",
    "    # Move to the next page\n",
    "    page += 1\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": product_descriptions,\n",
    "    \"Price\": prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7063882",
   "metadata": {},
   "source": [
    "5)Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "\n",
    "place=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac197fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5              Awesome   \n",
      "1       5  Best in the market!   \n",
      "2       5       Classy product   \n",
      "3       5    Worth every penny   \n",
      "4       5             Terrific   \n",
      "..    ...                  ...   \n",
      "95      5     Perfect product!   \n",
      "96      5            Fabulous!   \n",
      "97      5               Super!   \n",
      "98      5        Great product   \n",
      "99      5    Terrific purchase   \n",
      "\n",
      "                                          Full Review  \n",
      "0   iPhone 11 is a good phone. Not a very big diff...  \n",
      "1                                Good CameraREAD MORE  \n",
      "2   Camera is awesomeBest battery backupA performe...  \n",
      "3   Feeling awesome after getting the delivery of ...  \n",
      "4                             Very very goodREAD MORE  \n",
      "..                                                ...  \n",
      "95                                V Good allREAD MORE  \n",
      "96  It’s very good battery life and display and vi...  \n",
      "97               Good product 👌I love iPhoneREAD MORE  \n",
      "98                            Purple is bestREAD MORE  \n",
      "99                        Value for money 🖤🖤READ MORE  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize variables to store data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Define the URL for the product reviews page\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Initialize review counter\n",
    "reviews_scraped = 0\n",
    "\n",
    "# Continue scraping until we have data for 100 reviews\n",
    "while reviews_scraped < 100:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all the review boxes on the page\n",
    "        review_boxes = soup.find_all(\"div\", class_=\"_27M-vq\")\n",
    "        \n",
    "        for review_box in review_boxes:\n",
    "            # Extract Rating\n",
    "            rating = review_box.find(\"div\", class_=\"_3LWZlK\")\n",
    "            ratings.append(rating.text.strip())\n",
    "            \n",
    "            # Extract Review Summary\n",
    "            review_summary = review_box.find(\"p\", class_=\"_2-N8zT\")\n",
    "            review_summaries.append(review_summary.text.strip())\n",
    "            \n",
    "            # Extract Full Review\n",
    "            full_review = review_box.find(\"div\", class_=\"t-ZTKy\")\n",
    "            full_reviews.append(full_review.text.strip())\n",
    "            \n",
    "            # Increase the counter\n",
    "            reviews_scraped += 1\n",
    "            \n",
    "            # Break if we have data for 100 reviews\n",
    "            if reviews_scraped == 100:\n",
    "                break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of reviews\n",
    "    next_button = soup.find(\"a\", class_=\"_1LKTO3\")\n",
    "    \n",
    "    # Check if there is a \"Next\" button\n",
    "    if next_button:\n",
    "        next_page_url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
    "        url = next_page_url\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Rating\": ratings,\n",
    "    \"Review Summary\": review_summaries,\n",
    "    \"Full Review\": full_reviews\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7be1a",
   "metadata": {},
   "source": [
    "6)Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize variables to store data\n",
    "brands = []\n",
    "product_descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Define the URL for the search results page\n",
    "url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "\n",
    "# Initialize sneaker counter\n",
    "sneakers_scraped = 0\n",
    "\n",
    "# Continue scraping until we have data for 100 sneakers\n",
    "while sneakers_scraped < 100:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all the sneaker listings on the page\n",
    "        listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "        \n",
    "        for listing in listings:\n",
    "            # Extract Brand\n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\")\n",
    "            brands.append(brand.text.strip())\n",
    "            \n",
    "            # Extract Product Description\n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\")\n",
    "            product_descriptions.append(description.text.strip())\n",
    "            \n",
    "            # Extract Price\n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\")\n",
    "            prices.append(price.text.strip())\n",
    "            \n",
    "            # Increase the counter\n",
    "            sneakers_scraped += 1\n",
    "            \n",
    "            # Break if we have data for 100 sneakers\n",
    "            if sneakers_scraped == 100:\n",
    "                break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of sneakers\n",
    "    next_button = soup.find(\"a\", class_=\"_1LKTO3\")\n",
    "    \n",
    "    # Check if there is a \"Next\" button\n",
    "    if next_button:\n",
    "        next_page_url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
    "        url = next_page_url\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": product_descriptions,\n",
    "    \"Price\": prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837554bc",
   "metadata": {},
   "source": [
    "7)Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86e824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Amazon.in\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Find the search field and input \"Laptop\"\n",
    "search_input = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_input.send_keys(\"Laptop\")\n",
    "\n",
    "# Click the search icon\n",
    "search_icon = driver.find_element(By.ID, \"nav-search-submit-button\")\n",
    "search_icon.click()\n",
    "\n",
    "# Wait for the results to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Set the CPU Type filter to \"Intel Core i7\"\n",
    "cpu_filter = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']/span/a/div\")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Wait for the filter to be applied\n",
    "time.sleep(3)\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Scrape data for the first 10 laptops\n",
    "laptop_results = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")\n",
    "for laptop_result in laptop_results[:10]:\n",
    "    # Extract Title\n",
    "    title_element = laptop_result.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "    titles.append(title_element.text.strip())\n",
    "    \n",
    "    # Extract Ratings\n",
    "    rating_element = laptop_result.find_element(By.XPATH, \".//span[@class='a-icon-alt']\")\n",
    "    ratings.append(rating_element.text.strip())\n",
    "    \n",
    "    # Extract Price\n",
    "    try:\n",
    "        price_element = laptop_result.find_element(By.XPATH, \".//span[@class='a-price-whole']\")\n",
    "        price_fraction_element = laptop_result.find_element(By.XPATH, \".//span[@class='a-price-fraction']\")\n",
    "        price = f\"Rs. {price_element.text.strip()}.{price_fraction_element.text.strip()}\"\n",
    "        prices.append(price)\n",
    "    except:\n",
    "        prices.append(\"Not available\")\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Title\": titles,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Price\": prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d749b3",
   "metadata": {},
   "source": [
    "8)Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08b6f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rama krishna\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a625d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open azquotes.com\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "# Click on \"Top Quotes\" link\n",
    "top_quotes_link = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Wait for the top quotes page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "quotes = []\n",
    "authors = []\n",
    "types_of_quotes = []\n",
    "\n",
    "# Scrape data for the top 1000 quotes\n",
    "quote_count = 0\n",
    "while quote_count < 1000:\n",
    "    # Get the current page source\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    # Find all the quote elements on the page\n",
    "    quote_elements = soup.find_all(\"div\", class_=\"title\")\n",
    "    \n",
    "    for quote_element in quote_elements:\n",
    "        # Extract Quote\n",
    "        quote = quote_element.text.strip()\n",
    "        quotes.append(quote)\n",
    "        \n",
    "        # Extract Author\n",
    "        author_element = quote_element.find_next(\"div\", class_=\"author\")\n",
    "        author = author_element.text.strip()\n",
    "        authors.append(author)\n",
    "        \n",
    "        # Extract Type of Quote\n",
    "        type_element = quote_element.find_next(\"div\", class_=\"aauthor\")\n",
    "        type_of_quote = type_element.text.strip()\n",
    "        types_of_quotes.append(type_of_quote)\n",
    "        \n",
    "        quote_count += 1\n",
    "        if quote_count == 1000:\n",
    "            break\n",
    "    \n",
    "    # Click the \"Next\" button to go to the next page\n",
    "    next_button = driver.find_element(By.XPATH, \"//div[@class='pagination']//a[text()='Next']\")\n",
    "    if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "        break\n",
    "    else:\n",
    "        next_button.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Quote\": quotes,\n",
    "    \"Author\": authors,\n",
    "    \"Type of Quote\": types_of_quotes\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5146d",
   "metadata": {},
   "source": [
    "9)Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open jagranjosh.com\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "\n",
    "# Click on \"GK\" option\n",
    "gk_option = driver.find_element(By.LINK_TEXT, \"GK\")\n",
    "gk_option.click()\n",
    "\n",
    "# Click on \"List of all Prime Ministers of India\" link\n",
    "pm_link = driver.find_element(By.LINK_TEXT, \"List of all Prime Ministers of India\")\n",
    "pm_link.click()\n",
    "\n",
    "# Create lists to store the scraped data\n",
    "names = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Scrape data for the respected former Prime Ministers\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "pm_table = soup.find(\"table\", class_=\"mb20\")\n",
    "if pm_table:\n",
    "    pm_rows = pm_table.find_all(\"tr\")[1:]  # Exclude the header row\n",
    "    for row in pm_rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 4:\n",
    "            name = columns[0].text.strip()\n",
    "            born_dead_info = columns[1].text.strip()\n",
    "            term = columns[2].text.strip()\n",
    "            remark = columns[3].text.strip()\n",
    "            \n",
    "            names.append(name)\n",
    "            born_dead.append(born_dead_info)\n",
    "            term_of_office.append(term)\n",
    "            remarks.append(remark)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    \"Name\": names,\n",
    "    \"Born-Dead\": born_dead,\n",
    "    \"Term of Office\": term_of_office,\n",
    "    \"Remarks\": remarks\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ce06f",
   "metadata": {},
   "source": [
    "10)Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83963003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send an HTTP GET request to the webpage\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the \"GK\" option in the menu and get its link\n",
    "    gk_option = soup.find(\"li\", {\"id\": \"menu-GK\"})\n",
    "    gk_link = gk_option.find(\"a\")[\"href\"]\n",
    "\n",
    "    # Send a new GET request to the GK page\n",
    "    gk_url = f\"https://www.jagranjosh.com{gk_link}\"\n",
    "    gk_response = requests.get(gk_url)\n",
    "\n",
    "    # Check if the request to the GK page was successful\n",
    "    if gk_response.status_code == 200:\n",
    "        # Parse the HTML content of the GK page\n",
    "        gk_soup = BeautifulSoup(gk_response.text, \"html.parser\")\n",
    "\n",
    "        # Find the link to \"List of all Prime Ministers of India\"\n",
    "        pm_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "\n",
    "        # Check if the link was found\n",
    "        if pm_link:\n",
    "            # Send a new GET request to the Prime Ministers page\n",
    "            pm_url = pm_link[\"href\"]\n",
    "            pm_response = requests.get(pm_url)\n",
    "\n",
    "            # Check if the request to the Prime Ministers page was successful\n",
    "            if pm_response.status_code == 200:\n",
    "                # Parse the HTML content of the Prime Ministers page\n",
    "                pm_soup = BeautifulSoup(pm_response.text, \"html.parser\")\n",
    "\n",
    "                # Find the table containing the data\n",
    "                pm_table = pm_soup.find(\"table\", {\"class\": \"mb20\"})\n",
    "\n",
    "                # Create lists to store the scraped data\n",
    "                names = []\n",
    "                born_dead = []\n",
    "                term_of_office = []\n",
    "                remarks = []\n",
    "\n",
    "                # Extract data from the table\n",
    "                for row in pm_table.find_all(\"tr\")[1:]:\n",
    "                    columns = row.find_all(\"td\")\n",
    "                    if len(columns) == 4:\n",
    "                        names.append(columns[0].get_text(strip=True))\n",
    "                        born_dead.append(columns[1].get_text(strip=True))\n",
    "                        term_of_office.append(columns[2].get_text(strip=True))\n",
    "                        remarks.append(columns[3].get_text(strip=True))\n",
    "\n",
    "                # Create a DataFrame\n",
    "                data = {\n",
    "                    \"Name\": names,\n",
    "                    \"Born-Dead\": born_dead,\n",
    "                    \"Term of Office\": term_of_office,\n",
    "                    \"Remarks\": remarks\n",
    "                }\n",
    "\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # Display the DataFrame\n",
    "                print(df)\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to fetch data from Prime Ministers page\")\n",
    "        else:\n",
    "            print(\"Link to 'List of all Prime Ministers of India' not found\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from GK page\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the main page\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
